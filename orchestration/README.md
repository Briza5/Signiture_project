# Orchestration

Tato sloÅ¾ka obsahuje konfiguraci a dokumentaci pro automatizaci (orchestraci) Stock Market Data Pipeline.

## DostupnÃ© Schedulery

### 1. GitHub Actions âœ… (ImplementovÃ¡no)
- **Cesta**: `.github/workflows/stocks-pipeline.yml`
- **Dokumentace**: `github-actions/setup.md`
- **Trigger**: Cron (8:00 UTC weekdays) + manuÃ¡lnÃ­ spuÅ¡tÄ›nÃ­
- **Cost**: $0 (Free tier: 2000 min/mÄ›sÃ­c)
- **Status**: âœ… Ready to use

### 2. Dagster Cloud (PlÃ¡novÃ¡no)
- **Status**: ðŸ”„ Not implemented yet
- **Cost**: Free tier available
- **Use case**: Advanced orchestration s UI a monitoring

### 3. Apache Airflow (PlÃ¡novÃ¡no)
- **Status**: ðŸ”„ Not implemented yet
- **Cost**: Self-hosted (compute costs)
- **Use case**: Production-grade orchestration

### 4. GCP Cloud Scheduler + Cloud Run (PlÃ¡novÃ¡no)
- **Status**: ðŸ”„ Not implemented yet
- **Cost**: Pay-per-use
- **Use case**: Native GCP integration

## Quick Start (GitHub Actions)

1. **Push projekt na GitHub**
   ```bash
   git remote add origin https://github.com/your-username/your-repo.git
   git push -u origin master
   ```

2. **Nastavit GitHub Secrets**
   - Viz `github-actions/setup.md`

3. **Enable GitHub Actions**
   - GitHub â†’ Settings â†’ Actions â†’ General â†’ Allow all actions

4. **PrvnÃ­ run**
   - GitHub â†’ Actions â†’ Stock Market Data Pipeline â†’ Run workflow

## Architektura

```
Trigger (Cron/Manual)
    â†“
Job 1: Ingestion (dlt)
    â”œâ”€ Setup Python environment
    â”œâ”€ Install dependencies
    â”œâ”€ Configure credentials
    â”œâ”€ Run stock_pipeline.py
    â””â”€ Upload logs
    â†“
Job 2: Transformation (dbt)
    â”œâ”€ Setup Python + dbt
    â”œâ”€ Configure credentials
    â”œâ”€ Run dbt build
    â””â”€ Upload artifacts
    â†“
Job 3: Notification
    â”œâ”€ Check status
    â””â”€ Create summary
```

## Monitoring

### GitHub Actions UI
- **Logs**: GitHub â†’ Actions â†’ workflow run â†’ job â†’ step logs
- **Artifacts**: Logs a dbt artifacts ke staÅ¾enÃ­
- **Notifications**: Email pÅ™i failure (GitHub nastavenÃ­)

### BigQuery Monitoring
- **Pipeline runs**: `stocks_raw.pipeline_runs` table
- **Data freshness**: Query max(date) from daily_prices
- **Costs**: BigQuery â†’ Billing dashboard

## Development vs Production

| Environment | Scheduler | Target | Dataset |
|------------|-----------|--------|---------|
| **Local** | Manual (`python stock_pipeline.py`) | `dev` | `stocks_dev` |
| **CI/CD** | GitHub Actions | `prod` | `stocks_dev` |
| **Future** | Dagster/Airflow | `prod` | `stocks_prod` |

## Troubleshooting

### GitHub Actions Fails
1. Check logs v GitHub Actions UI
2. Verify secrets jsou nastavenÃ©
3. Check BigQuery permissions
4. Test pipeline lokÃ¡lnÄ›

### dbt Build Fails
1. Check dbt artifacts (manifest.json, run_results.json)
2. Verify credentials path
3. Run `dbt debug` lokÃ¡lnÄ›
4. Check BigQuery dataset existence

## Next Steps

- [ ] Add Slack/Email notifications
- [ ] Implement Dagster for better UI
- [ ] Add data quality monitoring
- [ ] Create production dataset (`stocks_prod`)
- [ ] Add cost alerts
