# GitHub Actions Setup Guide

KompletnÃ­ nÃ¡vod pro nastavenÃ­ automatizovanÃ© orchestrace pomocÃ­ GitHub Actions.

## Prerekvizity

- âœ… GitHub account
- âœ… Git repository inicializovanÃ½ v projektu
- âœ… BigQuery service account s credentials JSON
- âœ… FunkÄnÃ­ lokÃ¡lnÃ­ pipeline (dlt + dbt)

## Krok 1: PÅ™ipravit Projekt pro GitHub

### 1.1 OvÄ›Å™it .gitignore

UjistÄ›te se, Å¾e citlivÃ© soubory NEJSOU v gitu:

```bash
# Check Å¾e tyto soubory nejsou tracked
git status

# MÄ›ly by bÃ½t ignored:
ingestion/.dlt/secrets.toml
ingestion/credentials/*.json
transformation/credentials/*.json
.venv/
```

### 1.2 VytvoÅ™it GitHub Repository

**Varianta A: NovÃ© repo**
```bash
# Na GitHubu: Create new repository (bez README, .gitignore)
# LokÃ¡lnÄ›:
git remote add origin https://github.com/your-username/stock-portfolio-pipeline.git
git branch -M master
git push -u origin master
```

**Varianta B: ExistujÃ­cÃ­ repo**
```bash
# Pokud uÅ¾ mÃ¡te remote:
git push origin master
```

---

## Krok 2: Nastavit GitHub Secrets

GitHub Secrets = Å¡ifrovanÃ© environment variables pro CI/CD

### 2.1 Navigace v GitHub UI

```
Your Repository â†’ Settings â†’ Secrets and variables â†’ Actions â†’ New repository secret
```

### 2.2 VytvoÅ™it Secrets

#### Secret 1: `BIGQUERY_CREDENTIALS`
```
Name: BIGQUERY_CREDENTIALS
Value: <celÃ½ obsah service account JSON souboru>
```

**Jak zÃ­skat:**
```bash
# Windows PowerShell
Get-Content "D:\OneDrive\Data engineer\Projekty\Signiture_project\ingestion\credentials\dwhhbbi-21142b907feb.json" | Set-Clipboard

# Pak Ctrl+V do GitHub Secret value field
```

**FormÃ¡t (pro referenci):**
```json
{
  "type": "service_account",
  "project_id": "dwhhbbi",
  "private_key_id": "...",
  "private_key": "-----BEGIN PRIVATE KEY-----\n...\n-----END PRIVATE KEY-----\n",
  "client_email": "...",
  "client_id": "...",
  ...
}
```

#### Secret 2: `GCP_PROJECT_ID`
```
Name: GCP_PROJECT_ID
Value: dwhhbbi
```

#### Secret 3: `BIGQUERY_PRIVATE_KEY`
```
Name: BIGQUERY_PRIVATE_KEY
Value: <private_key z JSON credentials>
```

**Jak extrahovat:**
```python
# Python one-liner
import json
with open('ingestion/credentials/dwhhbbi-21142b907feb.json') as f:
    data = json.load(f)
    print(data['private_key'])

# ZkopÃ­ruj output vÄetnÄ› -----BEGIN PRIVATE KEY----- a -----END PRIVATE KEY-----
```

#### Secret 4: `BIGQUERY_CLIENT_EMAIL`
```
Name: BIGQUERY_CLIENT_EMAIL
Value: <client_email z JSON credentials>
```

**Jak extrahovat:**
```python
# Python
import json
with open('ingestion/credentials/dwhhbbi-21142b907feb.json') as f:
    data = json.load(f)
    print(data['client_email'])

# Output: something@dwhhbbi.iam.gserviceaccount.com
```

### 2.3 Verify Secrets

Po pÅ™idÃ¡nÃ­ vÅ¡ech secrets:
```
Settings â†’ Secrets and variables â†’ Actions
```

MÄ›li byste vidÄ›t:
- `BIGQUERY_CREDENTIALS`
- `GCP_PROJECT_ID`
- `BIGQUERY_PRIVATE_KEY`
- `BIGQUERY_CLIENT_EMAIL`

âš ï¸ **Pozor**: NemÅ¯Å¾ete zobrazit hodnotu secrets po uloÅ¾enÃ­ (jen nÃ¡zev)

---

## Krok 3: Enable GitHub Actions

### 3.1 Repository Settings

```
Settings â†’ Actions â†’ General
```

### 3.2 Permissions

**Workflow permissions:**
- â˜‘ï¸ Read and write permissions (pro uploading artifacts)

**Actions permissions:**
- â˜‘ï¸ Allow all actions and reusable workflows

### 3.3 Verify Workflow File

Zkontrolujte Å¾e `.github/workflows/stocks-pipeline.yml` existuje:

```bash
ls .github/workflows/
# Output: stocks-pipeline.yml
```

---

## Krok 4: PrvnÃ­ Test Run

### 4.1 Manual Trigger (DoporuÄeno pro prvnÃ­ run)

```
GitHub â†’ Actions tab â†’ "Stock Market Data Pipeline" â†’ Run workflow
```

**Options:**
- Branch: `master`
- Full refresh: `â˜` (unchecked pro incremental)

Click **"Run workflow"** ğŸš€

### 4.2 Sledovat Progress

```
Actions â†’ workflow run (ÄerstvÃ½ bÄ›h)
```

**Live view:**
- âœ… Green = success
- âŒ Red = failure
- ğŸŸ¡ Yellow = running

**Expand jobs** pro details:
1. Ingest Stock Data (dlt)
2. Transform Data (dbt)
3. Send Notifications

### 4.3 Check Artifacts

Po dokonÄenÃ­:
```
Workflow run â†’ Artifacts (scroll down)
```

Download:
- `pipeline-logs` - dlt execution logs
- `dbt-artifacts` - dbt manifest & results

---

## Krok 5: Verify v BigQuery

### 5.1 Check Data Freshness

```sql
-- OvÄ›Å™ Å¾e data byla naÄtena
SELECT
    MAX(date) as latest_date,
    COUNT(*) as row_count
FROM `dwhhbbi.stocks_raw.daily_prices`;
```

Expected:
- `latest_date` = dnes nebo vÄera (zÃ¡leÅ¾Ã­ na market open)
- `row_count` > 0

### 5.2 Check dbt Models

```sql
-- Verify marts layer
SELECT COUNT(*)
FROM `dwhhbbi.stocks_dev_marts.fct_daily_stock_performance`;
```

---

## Krok 6: Scheduled Runs (Automatizace)

### 6.1 Cron Schedule

Workflow se automaticky spustÃ­:
- **Kdy**: KaÅ¾dÃ½ vÅ¡ednÃ­ den v 8:00 AM UTC
- **Timezone**: UTC (= 9:00 CET v zimÄ›, 10:00 CEST v lÃ©tÄ›)
- **Cron**: `0 8 * * 1-5`

### 6.2 Adjust Schedule (Optional)

Editovat `.github/workflows/stocks-pipeline.yml`:

```yaml
schedule:
  # PÅ™Ã­klad: ZmÄ›na na 6:00 AM UTC
  - cron: '0 6 * * 1-5'
```

**Cron syntax reference:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ minute (0 - 59)
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ hour (0 - 23)
â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ day of month (1 - 31)
â”‚ â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ month (1 - 12)
â”‚ â”‚ â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ day of week (0 - 6) (Sunday to Saturday)
â”‚ â”‚ â”‚ â”‚ â”‚
â”‚ â”‚ â”‚ â”‚ â”‚
* * * * *
```

---

## Troubleshooting

### Problem: "Secrets not found"

**Symptom**: Workflow failuje s "Required secrets not set"

**Fix:**
1. Verify secrets existence: Settings â†’ Secrets
2. Check jmÃ©na secrets (case-sensitive)
3. Re-create secret pokud mÃ¡ typo

### Problem: BigQuery authentication failed

**Symptom**: "Could not authenticate to BigQuery"

**Fix:**
1. Verify `BIGQUERY_CREDENTIALS` obsahuje platnÃ½ JSON
2. Check service account mÃ¡ permissions:
   - BigQuery Data Editor
   - BigQuery Job User
   - BigQuery Read Session User

### Problem: dbt build fails

**Symptom**: `dbt build` krok failuje

**Fix:**
1. Check dbt artifacts pro error message
2. Verify `prod` target v `profiles.yml`
3. Test `dbt debug` lokÃ¡lnÄ›
4. Check environment variable `DBT_BIGQUERY_KEYFILE`

### Problem: Workflow nenÃ­ visible

**Symptom**: NevidÃ­m workflow v Actions tab

**Fix:**
1. Verify `.github/workflows/stocks-pipeline.yml` je committed
2. Push na GitHub: `git push origin master`
3. Wait 1-2 minuty pro GitHub indexing
4. Refresh Actions tab

---

## Notifications Setup (Optional)

### Email Notifications

GitHub automaticky poÅ¡le email pÅ™i workflow failure pokud:
1. Settings â†’ Notifications â†’ Actions
2. â˜‘ï¸ "Send notifications for failed workflows"

### Slack Notifications (Advanced)

PÅ™idat Slack webhook do workflow:

```yaml
# V notify job, pÅ™idat step:
- name: Slack Notification
  if: failure()
  uses: slackapi/slack-github-action@v1
  with:
    webhook-url: ${{ secrets.SLACK_WEBHOOK_URL }}
    payload: |
      {
        "text": "âŒ Stock Pipeline Failed!",
        "blocks": [
          {
            "type": "section",
            "text": {
              "type": "mrkdwn",
              "text": "*Pipeline Status:* Failed\n*Run:* ${{ github.run_id }}"
            }
          }
        ]
      }
```

---

## Security Best Practices

âœ… **DO:**
- Store vÅ¡echny credentials v GitHub Secrets
- Use environment variables v code
- Rotate service account keys pravidelnÄ›
- Limit service account permissions (principle of least privilege)

âŒ **DON'T:**
- Commit credentials do git (ani v .env files)
- Share secrets pÅ™es email/Slack
- Use personal accounts mÃ­sto service accounts
- Hardcode credentials v kÃ³du

---

## Cost Monitoring

### GitHub Actions Free Tier
- **Limit**: 2000 minutes/mÄ›sÃ­c
- **Estimated usage**: ~10 min/den = ~220 min/mÄ›sÃ­c
- **Overhead**: âœ… Plenty of headroom

### BigQuery Costs
- **Query costs**: ~$0.01/den (small dataset)
- **Storage costs**: ~$0.02/GB/mÄ›sÃ­c
- **Total estimated**: < $5/mÄ›sÃ­c

**Monitor:**
```
GCP Console â†’ Billing â†’ Reports
Filter: Product = BigQuery
```

---

## Next Steps

Po ÃºspÄ›Å¡nÃ©m setup:

1. âœ… Verify scheduled runs fungujÃ­
2. âœ… Monitor pipeline logs v Actions UI
3. âœ… Check BigQuery data freshness daily
4. ğŸ”„ Consider Slack notifications
5. ğŸ”„ Add data quality tests
6. ğŸ”„ Create production dataset (`stocks_prod`)

---

## Reference

- [GitHub Actions Documentation](https://docs.github.com/en/actions)
- [GitHub Encrypted Secrets](https://docs.github.com/en/actions/security-guides/encrypted-secrets)
- [Cron Syntax](https://crontab.guru/)
- [BigQuery Service Accounts](https://cloud.google.com/bigquery/docs/authentication/service-account-file)
