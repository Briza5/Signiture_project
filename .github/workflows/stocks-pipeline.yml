# =============================================================================
# GitHub Actions Workflow: Stock Market Data Pipeline
# =============================================================================
# Purpose: Automatizace daily stock data ingestion (dlt) a transformation (dbt)
# Trigger: Ka≈æd√Ω v≈°edn√≠ den v 8:00 UTC + manu√°ln√≠ spu≈°tƒõn√≠
# Architecture: Ingestion ‚Üí Transformation ‚Üí Notification (3 jobs v s√©rii)
# =============================================================================

name: Stock Market Data Pipeline

# -----------------------------------------------------------------------------
# WORKFLOW TRIGGERS (kdy se workflow spust√≠)
# -----------------------------------------------------------------------------
on:
  # Cron schedule - automatick√© spou≈°tƒõn√≠ v pravideln√Ωch intervalech
  schedule:
    # Cron syntax: 'minute hour day-of-month month day-of-week'
    # '0 8 * * 1-5' = ka≈æd√Ω v≈°edn√≠ den (Monday-Friday) v 8:00 AM UTC
    # D≈Øvod: Po uzav≈ôen√≠ US trh≈Ø (16:00 EST = 21:00 UTC), data dostupn√° n√°sleduj√≠c√≠ den
    - cron: '0 8 * * 1-5'

  # Manual trigger - umo≈æ≈àuje spustit workflow ruƒçnƒõ z GitHub UI
  workflow_dispatch:
    # Input parametry pro manu√°ln√≠ spu≈°tƒõn√≠
    inputs:
      full_refresh:
        description: 'Run full refresh (drops state and reloads all data)'
        required: false  # Parametr je voliteln√Ω
        type: boolean    # Checkbox v UI
        default: false   # Default = incremental load

# -----------------------------------------------------------------------------
# ENVIRONMENT VARIABLES (glob√°ln√≠ promƒõnn√© pro v≈°echny jobs)
# -----------------------------------------------------------------------------
env:
  PYTHON_VERSION: '3.11'  # Konzistentn√≠ Python verze pro v≈°echny kroky
  DBT_PROFILES_DIR: ./transformation  # Kde dbt hled√° profiles.yml

# =============================================================================
# JOB 1: INGESTION (dlt pipeline - fetch data z yfinance ‚Üí BigQuery)
# =============================================================================
jobs:
  ingest-data:
    name: Ingest Stock Data (dlt)

    # Runner environment - GitHub-hosted Ubuntu VM (free tier: 2000 min/mƒõs√≠c)
    runs-on: ubuntu-latest

    steps:
      # -------------------------------------------------------------------------
      # STEP 1: Checkout repository code
      # -------------------------------------------------------------------------
      # D≈Øvod: Actions runner je pr√°zdn√Ω VM, pot≈ôebujeme st√°hnout n√°≈° k√≥d
      # uses: p≈ôedp≈ôipraven√° GitHub action (actions/checkout verze 4)
      - name: Checkout code
        uses: actions/checkout@v4

      # -------------------------------------------------------------------------
      # STEP 2: Setup Python environment
      # -------------------------------------------------------------------------
      # D≈Øvod: Instalace Python a pip pro spou≈°tƒõn√≠ dlt pipeline
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}  # Pou≈æije promƒõnnou z env
          # cache: 'pip' - cachuje pip dependencies pro rychlej≈°√≠ bƒõhy
          # GitHub automaticky cachuje ~/.cache/pip na z√°kladƒõ requirements.txt hash
          cache: 'pip'

      # -------------------------------------------------------------------------
      # STEP 3: Install Python dependencies
      # -------------------------------------------------------------------------
      # D≈Øvod: Instalace dlt, yfinance, pandas a dal≈°√≠ch z√°vislost√≠
      - name: Install dependencies
        run: |
          pip install --upgrade pip  # Nejnovƒõj≈°√≠ pip pro rychlej≈°√≠ instalaci
          pip install -r requirements.txt  # Instalace v≈°ech project dependencies

      # -------------------------------------------------------------------------
      # STEP 4: Create BigQuery credentials file
      # -------------------------------------------------------------------------
      # D≈Øvod: dlt pot≈ôebuje service account JSON pro autentizaci k BigQuery
      # secrets.BIGQUERY_CREDENTIALS = JSON obsah service account key
      # Bezpeƒçnost: Secrets jsou encrypted v GitHub, nikdy nejsou v logu vidƒõt
      # KRITICK√â: mkdir -p MUS√ç b√Ωt p≈ôed vytvo≈ôen√≠m souboru!
      - name: Create BigQuery credentials file
        run: |
          mkdir -p ingestion/credentials
          echo '${{ secrets.BIGQUERY_CREDENTIALS }}' > ingestion/credentials/dwhhbbi-credentials.json

      # -------------------------------------------------------------------------
      # STEP 5: Configure dlt secrets + Set GOOGLE_APPLICATION_CREDENTIALS
      # -------------------------------------------------------------------------
      # D≈Øvod: dlt ƒçte credentials z .dlt/secrets.toml (nen√≠ v git)
      # PROBL√âM: private_key obsahuje newlines (control chars) ‚Üí TOML parse error
      # ≈òE≈†EN√ç: Pou≈æ√≠t Google Cloud standard environment variable
      #         GOOGLE_APPLICATION_CREDENTIALS = path to JSON file
      #         dlt BigQuery destination automaticky najde credentials z t√©to env var
      - name: Configure dlt secrets
        run: |
          mkdir -p ingestion/.dlt
          # Minim√°ln√≠ secrets.toml - pouze location
          # Credentials najde dlt z GOOGLE_APPLICATION_CREDENTIALS env var
          echo "[destination.bigquery]" > ingestion/.dlt/secrets.toml
          echo 'location = "US"' >> ingestion/.dlt/secrets.toml
          # Export GOOGLE_APPLICATION_CREDENTIALS pro n√°sleduj√≠c√≠ steps
          # Absolutn√≠ cesta k JSON credentials file vytvo≈ôen√©mu v Step 4
          echo "GOOGLE_APPLICATION_CREDENTIALS=${{ github.workspace }}/ingestion/credentials/dwhhbbi-credentials.json" >> $GITHUB_ENV

      # -------------------------------------------------------------------------
      # STEP 6: Run dlt pipeline (hlavn√≠ ingestion logika)
      # -------------------------------------------------------------------------
      # D≈Øvod: Fetch stock data z yfinance a load do BigQuery
      # working-directory: Mƒõn√≠ pwd pro tento krok
      # Conditional logic: full_refresh pokud user klikl checkbox, jinak incremental
      - name: Run dlt pipeline
        working-directory: ./ingestion
        run: |
          if [ "${{ github.event.inputs.full_refresh }}" == "true" ]; then
            python stock_pipeline.py --full-refresh
          else
            python stock_pipeline.py
          fi

      # -------------------------------------------------------------------------
      # STEP 7: Upload pipeline logs as artifacts
      # -------------------------------------------------------------------------
      # D≈Øvod: Zachov√°n√≠ log≈Ø pro debugging (viditeln√© v GitHub Actions UI)
      # if: always() - spust√≠ se i kdy≈æ p≈ôedchoz√≠ step failnul
      # retention-days: Logs se automaticky sma≈æou po 30 dnech
      # POZN√ÅMKA: logs/ slo≈æka je v .gitignore, tak≈æe nen√≠ v repository
      #           stock_pipeline.py vytv√°≈ô√≠ logs/ slo≈æku automaticky (mkdir exist_ok=True)
      #           Pokud pipeline failuje P≈òED vytvo≈ôen√≠m log≈Ø, artifact bude pr√°zdn√Ω
      # if-no-files-found: warn - pouze warning m√≠sto error pokud logy neexistuj√≠
      - name: Upload pipeline logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: pipeline-logs
          path: logs/pipeline_run_*.log
          retention-days: 30
          if-no-files-found: warn  # Jen warning pokud logy neexistuj√≠ (ne error)

# =============================================================================
# JOB 2: TRANSFORMATION (dbt - transform raw data ‚Üí analytics models)
# =============================================================================
  transform-data:
    name: Transform Data (dbt)
    runs-on: ubuntu-latest

    # needs: ingest-data - ƒçek√° dokud job 1 √∫spƒõ≈°nƒõ nedobƒõhne
    # D≈Øvod: Nem≈Ø≈æeme transformovat data, kter√° je≈°tƒõ nebyla ingestov√°na
    needs: ingest-data

    steps:
      # -------------------------------------------------------------------------
      # STEP 1: Checkout code (znovu, ka≈æd√Ω job bƒõ≈æ√≠ na nov√©m VM)
      # -------------------------------------------------------------------------
      - name: Checkout code
        uses: actions/checkout@v4

      # -------------------------------------------------------------------------
      # STEP 2: Setup Python (pro dbt-bigquery)
      # -------------------------------------------------------------------------
      # D≈Øvod: dbt je Python package, pot≈ôebujeme Python environment
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      # -------------------------------------------------------------------------
      # STEP 3: Install dbt-bigquery
      # -------------------------------------------------------------------------
      # D≈Øvod: dbt-bigquery adapter pro komunikaci s BigQuery
      # >=1.7.0 - minim√°ln√≠ verze pro podporu Fusion features
      - name: Install dbt dependencies
        run: |
          pip install --upgrade pip
          pip install dbt-bigquery>=1.7.0

      # -------------------------------------------------------------------------
      # STEP 4: Create BigQuery credentials for dbt
      # -------------------------------------------------------------------------
      # D≈Øvod: dbt profiles.yml pou≈æ√≠v√° environment variable DBT_BIGQUERY_KEYFILE
      # Vytv√°≈ô√≠me credentials ve stejn√©m m√≠stƒõ jako dlt (ingestion/credentials/)
      # OPRAVA: P≈Øvodnƒõ jsem vytv√°≈ôel v transformation/credentials/, ale:
      #   - ingestion/credentials/ je spr√°vn√© m√≠sto (konzistence s dlt)
      #   - profiles.yml prod target pou≈æ√≠v√° env var m√≠sto hardcoded path
      - name: Create BigQuery credentials for dbt
        run: |
          mkdir -p ingestion/credentials
          echo '${{ secrets.BIGQUERY_CREDENTIALS }}' > ingestion/credentials/dwhhbbi-credentials.json
          # Export path as environment variable pro dbt profiles.yml
          echo "DBT_BIGQUERY_KEYFILE=${{ github.workspace }}/ingestion/credentials/dwhhbbi-credentials.json" >> $GITHUB_ENV

      # -------------------------------------------------------------------------
      # STEP 5: Install dbt packages (dbt_utils, codegen)
      # -------------------------------------------------------------------------
      # D≈Øvod: Instalace dependencies z packages.yml
      # dbt deps = ekvivalent npm install nebo pip install -r requirements.txt
      - name: Install dbt packages
        working-directory: ./transformation
        run: dbt deps

      # -------------------------------------------------------------------------
      # STEP 6: Run dbt build (run + test v jednom p≈ô√≠kazu)
      # -------------------------------------------------------------------------
      # D≈Øvod: Build v≈°echny modely (staging ‚Üí intermediate ‚Üí marts) a spus≈• testy
      # --profiles-dir . = pou≈æij profiles.yml v current directory
      # --target prod = pou≈æij production target (stocks_dev dataset)
      # dbt build = run models + test v dependency order
      - name: Run dbt models
        working-directory: ./transformation
        run: dbt build --profiles-dir . --target prod

      # -------------------------------------------------------------------------
      # STEP 7: Generate dbt documentation
      # -------------------------------------------------------------------------
      # D≈Øvod: Vytvo≈ô√≠ interaktivn√≠ HTML dokumentaci s lineage, schemas, tests
      # V√Ωstup: target/index.html + manifest.json + catalog.json
      # catalog.json obsahuje column-level metadata z BigQuery information_schema
      - name: Generate dbt docs
        working-directory: ./transformation
        run: dbt docs generate --profiles-dir . --target prod

      # -------------------------------------------------------------------------
      # STEP 8: Install Elementary CLI and generate data quality report
      # -------------------------------------------------------------------------
      # D≈Øvod: Elementary Report poskytuje data quality monitoring, anomaly detection
      # elementary-data[bigquery] = edr CLI tool + BigQuery support
      # edr report = generuje elementary_report.html s test results a anomaliemi
      - name: Install Elementary CLI
        run: pip install 'elementary-data[bigquery]'

      - name: Generate Elementary report
        working-directory: ./transformation
        run: edr report --profiles-dir . --profile-target prod

      # -------------------------------------------------------------------------
      # STEP 9: Upload documentation for GitHub Pages
      # -------------------------------------------------------------------------
      # D≈Øvod: P≈ôipravit dbt docs + Elementary report pro publikaci
      # dbt docs: target/index.html + manifest.json + catalog.json + graph.gpickle
      # Elementary: edr_target/elementary_report.html
      # Note: compiled/ SQL nen√≠ pot≈ôeba pro viewing docs (jen pro debugging)
      - name: Upload documentation artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: documentation
          path: |
            transformation/target/index.html
            transformation/target/manifest.json
            transformation/target/catalog.json
            transformation/target/graph.gpickle
            transformation/edr_target/elementary_report.html
          retention-days: 30

# =============================================================================
# JOB 3: DEPLOY DOCUMENTATION (publish dbt docs + Elementary report to GitHub Pages)
# =============================================================================
  deploy-docs:
    name: Deploy Documentation to GitHub Pages
    runs-on: ubuntu-latest

    # needs: transform-data - ƒçek√° dokud transformation job √∫spƒõ≈°nƒõ nedobƒõhne
    # only: success() - spust√≠ se pouze pokud transform-data job uspƒõl (m√°me co publikovat)
    needs: transform-data
    if: success()

    # GitHub Pages environment - nutn√© pro deployment API
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}

    # GitHub Pages permissions - nutn√© pro deployment
    permissions:
      pages: write
      id-token: write

    steps:
      # -------------------------------------------------------------------------
      # STEP 1: Download documentation artifacts from transform-data job
      # -------------------------------------------------------------------------
      # D≈Øvod: Ka≈æd√Ω job bƒõ≈æ√≠ na nov√©m VM, mus√≠me st√°hnout artifacts z p≈ôedchoz√≠ho jobu
      - name: Download documentation artifacts
        uses: actions/download-artifact@v4
        with:
          name: documentation
          path: ./docs

      # -------------------------------------------------------------------------
      # STEP 2: Prepare GitHub Pages structure
      # -------------------------------------------------------------------------
      # D≈Øvod: Vytvo≈ôen√≠ user-friendly struktury pro GitHub Pages
      # Struktura:
      #   / = dbt docs (index.html, manifest.json, catalog.json)
      #   /elementary.html = Elementary data quality report
      - name: Prepare GitHub Pages content
        run: |
          # Debug: Show artifact structure after download
          echo "=== Downloaded artifact structure ==="
          ls -la docs/

          mkdir -p gh-pages
          # Disable Jekyll processing (dbt docs may have _folders that Jekyll would ignore)
          touch gh-pages/.nojekyll
          # Copy dbt docs to root (homepage)
          # Note: Artifact structure is docs/target/*, NOT docs/transformation/target/*
          # because upload-artifact finds least common ancestor (transformation/)
          cp -r docs/target/* gh-pages/

          # Copy Elementary report to root as elementary.html (with fallback)
          if [ -f docs/edr_target/elementary_report.html ]; then
            cp docs/edr_target/elementary_report.html gh-pages/elementary.html
            ELEMENTARY_STATUS="‚úÖ Available"
          else
            echo "<html><body><h1>Elementary Report Not Available</h1><p>Report generation failed or was skipped.</p></body></html>" > gh-pages/elementary.html
            ELEMENTARY_STATUS="‚ö†Ô∏è Not Generated"
          fi

          # Create simple README for GitHub Pages
          echo "# Stock Pipeline Documentation" > gh-pages/README.md
          echo "" >> gh-pages/README.md
          echo "- [dbt Documentation](index.html) - Data models, lineage, and tests" >> gh-pages/README.md
          echo "- [Elementary Report](elementary.html) - Data quality monitoring ($ELEMENTARY_STATUS)" >> gh-pages/README.md

      # -------------------------------------------------------------------------
      # STEP 3: Setup GitHub Pages
      # -------------------------------------------------------------------------
      # D≈Øvod: Konfigurace GitHub Pages pro deployment
      # Ofici√°ln√≠ GitHub action - pou≈æ√≠v√° Pages deployment API (ne gh-pages branch)
      - name: Setup GitHub Pages
        uses: actions/configure-pages@v5

      # -------------------------------------------------------------------------
      # STEP 4: Upload artifact for GitHub Pages
      # -------------------------------------------------------------------------
      # D≈Øvod: Upload p≈ôipraven√©ho obsahu jako Pages artifact
      # actions/upload-pages-artifact - speci√°ln√≠ artifact pro GitHub Pages
      - name: Upload GitHub Pages artifact
        uses: actions/upload-pages-artifact@v3
        with:
          path: ./gh-pages

      # -------------------------------------------------------------------------
      # STEP 5: Deploy to GitHub Pages
      # -------------------------------------------------------------------------
      # D≈Øvod: Publikace dokumentace na https://<username>.github.io/<repo>/
      # actions/deploy-pages - ofici√°ln√≠ GitHub Pages deployment (ne git push)
      # V√Ωstup: page_url = URL publikovan√© dokumentace
      - name: Deploy to GitHub Pages
        id: deployment
        uses: actions/deploy-pages@v4

# =============================================================================
# JOB 4: NOTIFICATION (check status a vytvo≈ô summary)
# =============================================================================
  notify:
    name: Send Notifications
    runs-on: ubuntu-latest

    # needs: [ingest-data, transform-data, deploy-docs] - ƒçek√° na v≈°echny p≈ôedchoz√≠ jobs
    # if: always() - spust√≠ se i kdy≈æ p≈ôedchoz√≠ jobs failnu (chceme notifikaci v≈ædy)
    needs: [ingest-data, transform-data, deploy-docs]
    if: always()

    steps:
      # -------------------------------------------------------------------------
      # STEP 1: Check overall pipeline status
      # -------------------------------------------------------------------------
      # D≈Øvod: Zjistit jestli nƒõjak√Ω job failnul
      # needs.<job>.result = success/failure/cancelled/skipped
      # exit 1 = fail tento step (pro triggering external notifications)
      # deploy-docs je optional - m≈Ø≈æe b√Ωt skipped pokud transform-data failnul
      - name: Check pipeline status
        run: |
          if [ "${{ needs.ingest-data.result }}" == "failure" ] || [ "${{ needs.transform-data.result }}" == "failure" ]; then
            echo "PIPELINE_STATUS=‚ùå FAILED" >> $GITHUB_ENV
            exit 1
          else
            echo "PIPELINE_STATUS=‚úÖ SUCCESS" >> $GITHUB_ENV
          fi

      # -------------------------------------------------------------------------
      # STEP 2: Create pipeline execution summary
      # -------------------------------------------------------------------------
      # D≈Øvod: Viditeln√Ω summary v GitHub Actions UI (p≈ôehledn√° reportace)
      # $GITHUB_STEP_SUMMARY = speci√°ln√≠ file, kter√Ω GitHub renderuje jako Markdown
      # V√Ωsledek: P≈ôehledn√° tabulka se statusy v≈°ech jobs + odkazy na GitHub Pages
      - name: Pipeline summary
        if: always()
        run: |
          echo "### Stock Pipeline Execution Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Status:** ${{ env.PIPELINE_STATUS }}" >> $GITHUB_STEP_SUMMARY
          echo "**Ingestion:** ${{ needs.ingest-data.result }}" >> $GITHUB_STEP_SUMMARY
          echo "**Transformation:** ${{ needs.transform-data.result }}" >> $GITHUB_STEP_SUMMARY
          echo "**Documentation:** ${{ needs.deploy-docs.result }}" >> $GITHUB_STEP_SUMMARY
          echo "**Triggered by:** ${{ github.event_name }}" >> $GITHUB_STEP_SUMMARY
          echo "**Run ID:** ${{ github.run_id }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          if [ "${{ needs.deploy-docs.result }}" == "success" ]; then
            echo "### üìö Documentation Links" >> $GITHUB_STEP_SUMMARY
            echo "- [dbt Docs](https://${{ github.repository_owner }}.github.io/${{ github.event.repository.name }}/)" >> $GITHUB_STEP_SUMMARY
            echo "- [Elementary Report](https://${{ github.repository_owner }}.github.io/${{ github.event.repository.name }}/elementary.html)" >> $GITHUB_STEP_SUMMARY
          fi

# =============================================================================
# WORKFLOW EXECUTION FLOW
# =============================================================================
# 1. Trigger: Cron (8:00 UTC weekdays) nebo manual
# 2. Job 1 (ingest-data):
#    - Checkout code
#    - Setup Python + dependencies
#    - Configure credentials
#    - Run dlt pipeline (yfinance ‚Üí BigQuery)
#    - Upload logs
# 3. Job 2 (transform-data):
#    - ƒåek√° na Job 1 success
#    - Checkout code
#    - Setup Python + dbt
#    - Configure credentials
#    - Run dbt build (raw ‚Üí staging ‚Üí intermediate ‚Üí marts)
#    - Generate dbt docs (index.html + manifest.json + catalog.json)
#    - Generate Elementary report (elementary_report.html)
#    - Upload artifacts
# 4. Job 3 (deploy-docs):
#    - ƒåek√° na Job 2 success
#    - Download documentation artifacts
#    - Prepare GitHub Pages structure (/ = dbt docs, /elementary.html = Elementary report)
#    - Deploy to GitHub Pages (using Pages deployment API, no gh-pages branch needed)
# 5. Job 4 (notify):
#    - ƒåek√° na Job 1, 2 & 3 (always runs)
#    - Check status
#    - Create summary with documentation links
#
# Celkov√Ω ƒças: ~5-10 minut (z√°vis√≠ na poƒçtu symbol≈Ø a dbt modelu)
# Cost: $0 (GitHub Actions free tier: 2000 min/mƒõs√≠c)
# GitHub Pages: https://<username>.github.io/<repo>/ (dbt docs + Elementary report)
# =============================================================================
